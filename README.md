# CMB Delensing with Deep Learning and Rotation Augmentation

A deep learning framework for Cosmic Microwave Background (CMB) delensing using U-Net architecture with rotation-based data augmentation. This project implements state-of-the-art techniques to reconstruct unlensed CMB maps from gravitationally lensed observations.

## ğŸŒŒ Project Overview

Gravitational lensing distorts the primordial CMB signal, making it challenging to extract cosmological information. This project uses deep learning to "delens" CMB maps, recovering the original unlensed signal. The approach employs:

- **U-Net Architecture**: A convolutional neural network optimized for image-to-image translation tasks
- **Rotation Augmentation**: Spherical rotation transformations to increase training data diversity
- **Multi-GPU Training**: Distributed training across multiple GPUs for faster convergence
- **HEALPix Integration**: Native support for HEALPix spherical pixelization scheme

## âœ¨ Key Features

- **ğŸ”„ Rotation Augmentation**: Advanced spherical rotation techniques for robust training
- **ğŸš€ Multi-GPU Support**: Distributed training on up to 8 GPUs
- **ğŸ’¾ Checkpoint Management**: Automatic saving and resuming of training sessions
- **ğŸ“Š Comprehensive Logging**: Detailed training metrics and loss tracking
- **ğŸ¯ Flexible Architecture**: Configurable U-Net depth and parameters
- **ğŸŒ HEALPix Compatible**: Full integration with HEALPix spherical data format

## ğŸ“‹ Requirements

### System Requirements
- Linux/Unix operating system
- NVIDIA GPU(s) with CUDA support
- Python 3.7+
- At least 48GB RAM (recommended for large datasets)

### Python Dependencies
```
numpy>=1.19.0
tensorflow>=2.8.0
scipy>=1.7.0
healpy>=1.15.0  # For HEALPix operations
matplotlib>=3.3.0  # For visualization
```

### Hardware Recommendations
- **GPU**:  Tesla A40
- **RAM**: 64GB+ for optimal performance
- **Storage**: SSD with >500GB free space for datasets and checkpoints

## ğŸ› ï¸ Installation

1. **Clone the Repository**
   ```bash
   git clone <repository-url>
   cd CMB_Delensing
   ```

2. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Verify GPU Setup**
   ```python
   import tensorflow as tf
   print("GPUs Available:", tf.config.list_physical_devices('GPU'))
   ```

## ğŸ“ Project Structure

```
CMB_Delensing/
â”œâ”€â”€ run_rot.py                 # Main training script
â”œâ”€â”€ unet/                      # U-Net model implementation
â”‚   â””â”€â”€ unet_2d.py            # 2D U-Net architecture
â”œâ”€â”€ train_dataset/            # Training data directory
â”‚   â”œâ”€â”€ Blen_5maps.npy       # Lensed B-mode maps
â”‚   â”œâ”€â”€ Bunl_5maps.npy       # Unlensed B-mode maps
â”‚   â”œâ”€â”€ Elen_5maps.npy       # Lensed E-mode maps
â”‚   â””â”€â”€ Eunl_5maps.npy       # Unlensed E-mode maps
â”œâ”€â”€ rots_npys/               # Rotation transformation arrays
â”‚   â”œâ”€â”€ rot_arr_*.npy        # Forward rotation arrays
â”‚   â””â”€â”€ rerot_arr_*.npy      # Inverse rotation arrays
â”œâ”€â”€ checkpoints/             # Model checkpoints
â”‚   â”œâ”€â”€ check_B/            # B-mode checkpoints
â”‚   â””â”€â”€ check_E/            # E-mode checkpoints
â”œâ”€â”€ result/                  # Training outputs
â””â”€â”€ README.md               # This file
```

## ğŸš€ Usage

### Basic Training Command

```bash
python run_rot.py <epochs> <field> <scale>
```

**Parameters:**
- `epochs`: Number of training epochs (e.g., 2000)
- `field`: CMB field type ("B" for B-mode, "E" for E-mode)
- `scale`: Scaling factor for output normalization (typically 1-10)

### Example Commands

```bash
# Train B-mode delensing for 2000 epochs with scale factor 5
python run_rot.py 2000 B 5

# Train E-mode delensing for 1500 epochs with scale factor 3  
python run_rot.py 1500 E 3

# Short test run
python run_rot.py 100 B 1
```

### Training Configuration

The script automatically configures:
- **Learning Rate Schedule**: Piecewise decay (1e-2 â†’ 1e-3 â†’ 1e-4)
- **Batch Size**: 16 (adjustable in code)
- **GPU Usage**: All available GPUs (0-7)
- **Rotation Parameters**: Predefined theta/phi combinations

### Resuming Training

Training automatically resumes from the last checkpoint if interrupted:
```bash
# Same command - will auto-resume from last checkpoint
python run_rot.py 2000 B 5
```

## âš™ï¸ Configuration Options

### Model Architecture Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `n_filters` | 256 | Number of filters in first layer |
| `network_depth` | 3 | U-Net encoder/decoder depth |
| `dropout` | 0.2 | Dropout rate for regularization |
| `growth_factor` | 2 | Filter multiplication factor per layer |

### Training Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `batch_size` | 16 | Training batch size |
| `learning_rate` | Scheduled | Piecewise constant decay |
| `loss` | 'logcosh' | Training loss function |
| `optimizer` | Adam | Optimization algorithm |

### Data Augmentation

The rotation augmentation uses 8 predefined combinations:
- **Theta**: [60Â°, 120Â°, 240Â°]  
- **Phi**: [-60Â°, -30Â°, 30Â°, 60Â°]

Currently uses combination index 2: `[120Â°, 30Â°]`

## ğŸ“Š Output Files

### Training Results
- `{field}_pred_epochs_{epochs}_{theta}_{phi}.npy`: Model predictions
- `{field}_test_epochs_{epochs}_{theta}_{phi}.npy`: Test labels  
- `{field}_label_epochs_{epochs}_{theta}_{phi}.npy`: Test inputs
- `{field}_loss_train_epochs_{epochs}_{theta}_{phi}.npy`: Training losses
- `{field}_loss_val_epochs_{epochs}_{theta}_{phi}.npy`: Validation losses

### Model Checkpoints
- `checkpoint_100.ckpt`: Model weights (saved every 100 epochs)
- `final_model.h5`: Complete trained model
- `training_status.npy`: Training state for resumption

## ğŸ“ˆ Monitoring Training

### Real-time Monitoring
```bash
# Monitor GPU usage
nvidia-smi -l 1

# Monitor training logs
tail -f training.log
```

### Performance Metrics
- **Training Loss**: Log-cosh loss on training set
- **Validation Loss**: Log-cosh loss on validation set  
- **MAE**: Mean Absolute Error
- **MSE**: Mean Squared Error

## ğŸ”§ Troubleshooting

### Common Issues

**1. Out of Memory Error**
```bash
# Reduce batch size in code
batch_size = 8  # Instead of 16
```

**2. CUDA Out of Memory**
```bash
# Limit GPU memory growth
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
```

**3. Import Errors**
```bash
# Ensure unet module is in Python path
export PYTHONPATH="${PYTHONPATH}:."
```

**4. Data Loading Issues**
- Verify dataset files exist in `train_dataset/`
- Check HEALPix rearrangement files in `/home/nisl/Data/CMB_DeLensing/`
- Ensure rotation arrays exist in `rots_npys/`

### Performance Optimization

**Multi-GPU Training**
```python
# Verify distributed strategy
strategy = tf.distribute.MirroredStrategy()
print(f"Number of devices: {strategy.num_replicas_in_sync}")
```

**Memory Optimization**
```python
# Enable mixed precision training
policy = tf.keras.mixed_precision.Policy('mixed_float16')
tf.keras.mixed_precision.set_global_policy(policy)
```

## ğŸ“š Technical Details

### Algorithm Overview

1. **Data Loading**: Load lensed/unlensed CMB map pairs
2. **Rotation Augmentation**: Apply spherical rotations using HEALPix
3. **Model Training**: Train U-Net to map lensed â†’ unlensed
4. **Inverse Rotation**: Transform predictions back to original coordinates
5. **Evaluation**: Compare predictions with ground truth

### U-Net Architecture

```
Input (512Ã—512Ã—1)
    â†“
Encoder: 3 levels of conv+pooling
    â†“
Bottleneck: Dense feature representation  
    â†“
Decoder: 3 levels of upconv+concat
    â†“
Output (512Ã—512Ã—1)
```

### Data Flow

```
Raw HEALPix â†’ Rotation â†’ 512Ã—512 Maps â†’ U-Net â†’ Predictions â†’ Inverse Rotation â†’ Results
```



## ğŸ“– References

- **HEALPix**: Hierarchical Equal Area isoLatitude Pixelization
- **U-Net**: Convolutional Networks for Biomedical Image Segmentation
- **CMB Lensing**: Gravitational lensing of the cosmic microwave background



## ğŸ‘¤ Author

**nisl**
- Cosmic Microwave Background research
- Deep learning applications in cosmology

